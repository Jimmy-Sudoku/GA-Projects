{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84df62f4",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Webscraping from Reddit : Depression vs Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2760dfb",
   "metadata": {},
   "source": [
    "# Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deaee84",
   "metadata": {},
   "source": [
    "The project was done as part of General Assembly's requirement to pass the course. The aim of this project is to identify and classify two different subreddit post using Natural Language Processing (NLP). To achieve it, I have to do webscraping, clean data and preprocess data, Exploratory Data Analysis (EDA) and training various model to predict and identify the subreddit post. A model accuracy of 96% was achieved for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8cf98",
   "metadata": {},
   "source": [
    "# Table of content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650da73f",
   "metadata": {},
   "source": [
    "- [Background](#Background)\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Assumptions](#Assumptions)\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Functions](#Functions)\n",
    "- [Web Scraping](#Web-Scraping)\n",
    "- [Data Cleaning and EDA](#Data-Cleaning-and-EDA)\n",
    "- [Modeling](#Modeling)\n",
    "- [Conclusions & Recommendations](#Conclusions-&-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a29aa",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a71b68",
   "metadata": {},
   "source": [
    "According to Bachmann S. Epidemiology of Suicide and the Psychiatric Perspective, most suicides are related to psychiatric disease, with depression, substance use disorders and psychosis being the most relevant risk factors. In view of this statistic, a newly developed social media application, Chipper, has implemented a new feature where users are able to report other users' posts for suspected mental health issue so that they will be able to provide help to these users before it is too late.\n",
    "As a data scientist working in this company, I am tasked to train a classifier that will categorise posts that were reported for mental health issues into either Anxiety or Depression so that we are able to route these users to its appropriate helpline. To train the classifier, I will be using posts from Reddit's r/Anxiety and r/Depression subreddits as proxy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb463e",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed42f8",
   "metadata": {},
   "source": [
    "To correctly classify post from the correct subreddit for Chipper to route the users to its appropriate helpline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e90e4",
   "metadata": {},
   "source": [
    "# Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408049af",
   "metadata": {},
   "source": [
    "The following are the assumptions made:\n",
    "<br>\n",
    "- all data analysed are based on the time where the data was scrapped,\n",
    "<br>\n",
    "- the target audiences consist of a mix of technical and non-technical background,\n",
    "<br>\n",
    "- due to the time and hardware limitations, we are unable to perform a live data monitoring and analysis,\n",
    "<br>\n",
    "- photos, vidoes and emojis from the post are ignored and removed from our analysis,\n",
    "<br>\n",
    "- missing data are removed from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed71d2",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d104844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt \n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from IPython.display import Image\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy as sp\n",
    "\n",
    "#Import EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import sklearn\n",
    "\n",
    "#Import Modeling libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve,plot_confusion_matrix\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7d56b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd18c1",
   "metadata": {},
   "source": [
    "## Webscraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5af6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_reddit(subreddit, no_of_post, days = 30):\n",
    "    \n",
    "    # Create a api link and store it in a api variable\n",
    "    api = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    \n",
    "    # Create a url link and store it in a url variable\n",
    "    url = f'{api}?subreddit={subreddit}&size=100'\n",
    "    \n",
    "    # Create an empty list to store posts in the post variable\n",
    "    posts = []\n",
    "    \n",
    "    # Change url after each iteration\n",
    "    for i in range(1, no_of_post+1):\n",
    "        urlmod = '{}&after={}d'.format(url, days*i)\n",
    "        res_1 = requests.get(urlmod)\n",
    "        \n",
    "        # Prevent errors from stopping the code\n",
    "        try:\n",
    "            results = requests.get(urlmod)\n",
    "            assert results.status_code == 200\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Converting to json\n",
    "        extracted = results.json()['data']\n",
    "        \n",
    "        # Change dataframe from dict\n",
    "        df = pd.DataFrame.from_dict(extracted)\n",
    "        \n",
    "        # Adding the df to post list(created on top)\n",
    "        posts.append(df)\n",
    "        \n",
    "        # Total scrapped posts\n",
    "        total_scraped = sum(len(x) for x in posts)\n",
    "        \n",
    "        # If there are more than n values/data, stop. \n",
    "        if total_scraped > no_of_post:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        # Generate a random sleep duration to prevent being blocked\n",
    "        sleep_duration = random.randint(1,9)\n",
    "        time.sleep(sleep_duration)\n",
    "            \n",
    "    \n",
    "    # Creating a list of features that we will be using\n",
    "    features_required = ['title','subreddit','selftext']\n",
    "    \n",
    "    # Merge all iterations into 1 dataframe\n",
    "    df_merged = pd.concat(posts, sort=False)\n",
    "    \n",
    "    # Select the columns that we want from the datasets\n",
    "    df_merged = df_merged[features_required]\n",
    "    \n",
    "    # Dropping any duplicates\n",
    "    df_merged.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df_merged.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da242da",
   "metadata": {},
   "source": [
    "## Cleaning Function : Check for unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43137cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purpose : Check for inconsistant data and unique values\n",
    "\n",
    "def inconsist_uni_val(dataframe):\n",
    "    \n",
    "    #Create a list to store updated column names\n",
    "    column_name=dataframe.columns[2:4]\n",
    "\n",
    "    #Create a loop to check each column data and unique values\n",
    "    for name in column_name:\n",
    "        display(name)\n",
    "        display(dataframe[name].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0796c0",
   "metadata": {},
   "source": [
    "## Cleaning Function : Lemmatization of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d5e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(text):\n",
    "    #Remove punctuation\n",
    "    #text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    # \\W matches any non-word character (equivalent to [^a-zA-Z0-9_]). This does not include spaces i.e. \\s\n",
    "    # Add a + just in case there are 2 or more spaces between certain words\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "    # Requires a full sentence to be passed in as opposed to a tokenized list\n",
    "    text = \" \".join([wn.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b036ac",
   "metadata": {},
   "source": [
    "## Cleaning Function : Removal of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525e85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    # store character only if it is not a punctuation\n",
    "    text_nopunct = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "    return text_nopunct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d15fbd",
   "metadata": {},
   "source": [
    "## Modeling : Scores, Metrics, ROC Curve and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b846ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(vect_name_for_display,model_name_for_display,model,X_train,y_train,X_test,y_test):\n",
    "    \n",
    "    #Fit model\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # save the model to disk\n",
    "    filename = f'{vect_name_for_display}-{model_name_for_display} model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    # Predicting the results using the X_train and X_test\n",
    "    y_train_pred=model.predict(X_train)\n",
    "    y_test_pred=model.predict(X_test)\n",
    "    \n",
    "    # Train and test accuracy scores\n",
    "    display(f'{vect_name_for_display} - {model_name_for_display} Accuracy Train score : {round(accuracy_score(y_train,y_train_pred),4)}')\n",
    "    display(f'{vect_name_for_display} - {model_name_for_display} Accuracy Test score : {round(accuracy_score(y_test,y_test_pred),4)}')\n",
    "    \n",
    "    # Train and test Roc scores\n",
    "    display(f'{vect_name_for_display} - {model_name_for_display} Roc-Auc Train score : {round(roc_auc_score(y_train,y_train_pred),4)}')\n",
    "    display(f'{vect_name_for_display} - {model_name_for_display} Roc-Auc Test score : {round(roc_auc_score(y_test,y_test_pred),4)}')\n",
    "        \n",
    "    # Display Classification Metrics\n",
    "    display(f'{vect_name_for_display} - {model_name_for_display} Classification Report : ')\n",
    "    print(classification_report(y_test,y_test_pred))\n",
    "    \n",
    "    # Plotting of the ROC curve\n",
    "    plot_roc_curve(model,X_test, y_test)\n",
    "    plt.plot([0, 1], [0, 1], label='baseline', linestyle='--')\n",
    "    plt.title(f\"{vect_name_for_display}-{model_name_for_display} ROC Curve\")\n",
    "    plt.legend();\n",
    "    plt.show()\n",
    "    \n",
    "    #Confusion Matrix\n",
    "    labels=[\"Depression\",\"Anxiety\"]\n",
    "    plot_confusion_matrix(model,X_test, y_test,cmap=plt.cm.Blues)\n",
    "    color = 'white'\n",
    "    plt.title(f'{vect_name_for_display} - {model_name_for_display} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e516c",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a404eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add date and time in the webscraping to know when was the last scrapped date\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b00272",
   "metadata": {},
   "source": [
    "## Depression and Anxiety "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b69049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "#The above code is used to prevent new data from being scrapped.\n",
    "\n",
    "#Datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "#Start timer to time the process\n",
    "start=time.perf_counter()\n",
    "\n",
    "#Use the function above to scrap the data\n",
    "submissions_depression_df = get_data_from_reddit('Depression', 3000)\n",
    "submissions_anxiety_df = get_data_from_reddit('Anxiety', 3000)\n",
    "\n",
    "#Show the results in words\n",
    "display(f'From Pushshift : Scrapped \\'depression\\' {len(submissions_depression_df)} posts on {dt_string}.')\n",
    "display(f'From Pushshift : Scrapped \\'anxiety\\' {len(submissions_anxiety_df)} posts on {dt_string}.')\n",
    "\n",
    "#End timer\n",
    "end=time.perf_counter() \n",
    "\n",
    "#Show the time taken to run the code\n",
    "display(f'It took {abs(round(start-end,2))} seconds to run the code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff5925",
   "metadata": {},
   "source": [
    "The webscraping code summary:\n",
    "<br>\n",
    "- it is last run and scrapped on 24/11/22, 1.06 pm,\n",
    "<br>\n",
    "- took 592.09. secs to run and\n",
    "<br>\n",
    "- a total of around 6000 subreddit posts were scrapped for both depression and anxiety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3662",
   "metadata": {},
   "source": [
    "## Save the scrapped files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09773df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "#The above code is used to prevent new data from being saved.\n",
    "\n",
    "submissions_depression_df.to_csv('../00-datasets/depression_data.csv')\n",
    "submissions_anxiety_df.to_csv('../00-datasets/anxiety_data.csv')\n",
    "\n",
    "#Show the results in words\n",
    "display(f'The depression and anxiety files was last saved on {dt_string}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2da7b",
   "metadata": {},
   "source": [
    "The depression and anxiety files was last saved on 24/11/22, 1.06pm . \n",
    "<br>\n",
    "Even though the code is linked from the next file, the data scrapped are unable to be brought over as there was a code to prevent it from running and everytime the file closes or restart, the data will be lost.\n",
    "<br>\n",
    "Therefore, we need to save it as csv so that we can import the datasets for analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ee7a686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 of the notebook has been synchronized. \n",
      "Please proceed to part 2 of the notebook. \n",
      "Thank you for your patience.\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 1 of the notebook has been synchronized. \\nPlease proceed to part 2 of the notebook. \\nThank you for your patience.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82d1d1",
   "metadata": {},
   "source": [
    "To continue, please proceed to [part 2](./02-Code-Part-02.ipynb) of the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
